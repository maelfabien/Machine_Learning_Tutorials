{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:3em\"> Improving Few-shot Text Classification </h1></center>\n",
    "<br>\n",
    "<center><i style=\"font-size:1.3em\">via Pretrained Language Representations</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources :\n",
    "- Includes Easy Data Augmentation (EDA) package : https://github.com/jasonwei20/eda_nlp/blob/master/code/eda.py introduced after this paper : https://arxiv.org/pdf/1901.11196.pdf\n",
    "- Neural Structure Learning for sentiment classification (learn graph from embedding) : https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb\n",
    "- RNN text classification with Tensorflow 2.0 : https://www.tensorflow.org/beta/tutorials/text/text_classification_rnn\n",
    "- Great paper from August 2019 : https://arxiv.org/pdf/1908.08788.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:28:53.160794Z",
     "start_time": "2019-09-10T08:28:42.136092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.21.1) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import seed\n",
    "from random import sample\n",
    "import random\n",
    "from random import shuffle\n",
    "import re\n",
    "\n",
    "seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import spatial\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.preprocessing.text as kpt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import neural_structured_learning as nsl\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.legacy.nn as luann\n",
    "import sys\n",
    "\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import Dataset\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import (\n",
    "        BertModel,\n",
    "        BertForNextSentencePrediction,\n",
    "        BertForMaskedLM,\n",
    "        BertForMultipleChoice,\n",
    "        BertForPreTraining,\n",
    "        BertForQuestionAnswering,\n",
    "        BertForSequenceClassification,\n",
    "        BertForTokenClassification,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:28:53.181519Z",
     "start_time": "2019-09-10T08:28:53.164058Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "'himself', 'she', 'her', 'hers', 'herself', \n",
    "'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "'theirs', 'themselves', 'what', 'which', 'who', \n",
    "'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "'because', 'as', 'until', 'while', 'of', 'at', \n",
    "'by', 'for', 'with', 'about', 'against', 'between',\n",
    "'into', 'through', 'during', 'before', 'after', \n",
    "'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "'out', 'on', 'off', 'over', 'under', 'again', \n",
    "'further', 'then', 'once', 'here', 'there', 'when', \n",
    "'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "'should', 'now', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:13.664314Z",
     "start_time": "2019-09-10T08:28:53.184262Z"
    }
   },
   "outputs": [],
   "source": [
    "#model2 = api.load('glove-twitter-25')\n",
    "model2 = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes from Stackoverflow Short Text Classification : https://github.com/jacoxu/StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:13.736223Z",
     "start_time": "2019-09-10T08:31:13.667243Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"stack.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:13.750565Z",
     "start_time": "2019-09-10T08:31:13.738681Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"’\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.012169Z",
     "start_time": "2019-09-10T08:31:13.753805Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: get_only_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.039451Z",
     "start_time": "2019-09-10T08:31:14.015366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how do i fill a dataset or a datatable from a ...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how do you page a collection with linq</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best subversion clients for windows vista bit</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>best practice collaborative environment bin di...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visual studio setup project per user registry ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  how do i fill a dataset or a datatable from a ...     18\n",
       "1            how do you page a collection with linq      18\n",
       "2     best subversion clients for windows vista bit       3\n",
       "3  best practice collaborative environment bin di...      3\n",
       "4  visual studio setup project per user registry ...      7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.046942Z",
     "start_time": "2019-09-10T08:31:14.042778Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "sample_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.068203Z",
     "start_time": "2019-09-10T08:31:14.050592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate samples that contains K samples of each class\n",
    "\n",
    "def gen_sample(sample_size, num_classes):\n",
    "    \n",
    "    df_1 = df[(df[\"Label\"]<num_classes + 1)].reset_index().drop([\"index\"], axis=1).reset_index().drop([\"index\"], axis=1)\n",
    "    train = df_1[df_1[\"Label\"] == np.unique(df_1['Label'])[0]].sample(sample_size)\n",
    "    \n",
    "    train_index = train.index.tolist()\n",
    "    \n",
    "    for i in range(1,num_classes):\n",
    "        train_2 = df_1[df_1[\"Label\"] == np.unique(df_1['Label'])[i]].sample(sample_size)\n",
    "        train = pd.concat([train, train_2], axis=0)\n",
    "        train_index.extend(train_2.index.tolist())\n",
    "        \n",
    "    test = df_1[~df_1.index.isin(train_index)]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.082050Z",
     "start_time": "2019-09-10T08:31:14.072133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Text processing (split, find token id, get embedidng)\n",
    "\n",
    "def transform_sentence(text, model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Mean embedding vector\n",
    "    \"\"\"\n",
    "    \n",
    "    def preprocess_text(raw_text, model=model):\n",
    "        \n",
    "        \"\"\" \n",
    "        Eexcluding unknown words and get corresponding token\n",
    "        \"\"\"\n",
    "        \n",
    "        raw_text = raw_text.split()\n",
    "        \n",
    "        return list(filter(lambda x: x in model.vocab, raw_text))\n",
    "    \n",
    "    tokens = preprocess_text(text)\n",
    "    \n",
    "    if not tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    text_vector = np.mean(model[tokens], axis=0)\n",
    "    \n",
    "    return np.array(text_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.125152Z",
     "start_time": "2019-09-10T08:31:14.085562Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = gen_sample(sample_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.145952Z",
     "start_time": "2019-09-10T08:31:14.130224Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>how to get wordpress page id after looping posts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>using wp query to pull content from a specific...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>wordpress how to show just posts on main index</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>wordpress is it possible to make one particula...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>exclude templates in wordpress page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>wierd date and time formating in wordpress y m d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>wordpress custom post type templates</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>inserting wordpress plugin content to posts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>how can i delay one feed in wordpress but not ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>how can i remove jquery from the frontside of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>wordpress calling recent posts widget via scri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>debuging register activation hook in wordpress</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>testing pluggable function calls clashes for w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>would it be quicker to make wordpress theme di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>how to find and clean wordpress from script s ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>wordpress set post date</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>wordpress add comment like stackoverflow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>wordpress nav not visible in pages like articl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>wordpress development</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>wordpress static pages how to embed content in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>filtering search results with wordpress</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>drop down js menu blinking in ie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>wrap stray text in p tags</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>wordpress menu of categories</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>wordpress menu with superslide show</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>getting post information outside the wordpress...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>why cant i include a blog</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>d slideshow for wordpress and url file access ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>submit wordpress form programmatically</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>move wordpress from home web server to web ser...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>how to use sqlab xpert tuning to tune sql for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>oracle connection problem on mac osx status fa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>xaconnection performance in oracle g</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>what is the pl sql api difference between orac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>how do i call an oracle function from oci</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>explain plan cost vs execution time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>user interface for creating oracle sql loader ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>is it possible to refer to column names via bi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>can i run an arbitrary oracle sql script throu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>oracle logon protocol o logon in g</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>databases oracle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>resultset logic when selecting tables without ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>oracle stored procedures sys refcursor and nhi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>oracle express edition can not connect remotel...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>performance of remote materialized views in or...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>merge output cursor of sp into table</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>oracle logon trigger not being fired</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>understanding lob segments sys lob in oracle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>oracle database character set issue with the a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>finding the days of the week within a date ran...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>how to convert sql server to oracle</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>how can i avoid ta warning fom an unused param...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>oracle hierarchical query how to include top l...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>oracle record history using as of timestamp wi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>how to call a function with rowtype parameter ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ssis oracle parameter mapping</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>multi line pl sql command with net oraclecommand</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>read write data from to a file in pl sql witho...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>oracle sql parsing a name string and convertin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>oracle optimizing query involving date calcula...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Label\n",
       "1521  how to get wordpress page id after looping posts       1\n",
       "1737  using wp query to pull content from a specific...      1\n",
       "1740     wordpress how to show just posts on main index      1\n",
       "1660  wordpress is it possible to make one particula...      1\n",
       "1411                exclude templates in wordpress page      1\n",
       "1678   wierd date and time formating in wordpress y m d      1\n",
       "1626               wordpress custom post type templates      1\n",
       "1513        inserting wordpress plugin content to posts      1\n",
       "1859  how can i delay one feed in wordpress but not ...      1\n",
       "1072  how can i remove jquery from the frontside of ...      1\n",
       "1811  wordpress calling recent posts widget via scri...      1\n",
       "721      debuging register activation hook in wordpress      1\n",
       "1636  testing pluggable function calls clashes for w...      1\n",
       "1973  would it be quicker to make wordpress theme di...      1\n",
       "1938  how to find and clean wordpress from script s ...      1\n",
       "1899                            wordpress set post date      1\n",
       "1280           wordpress add comment like stackoverflow      1\n",
       "1883  wordpress nav not visible in pages like articl...      1\n",
       "1761                             wordpress development       1\n",
       "1319  wordpress static pages how to embed content in...      1\n",
       "1549            filtering search results with wordpress      1\n",
       "1174                  drop down js menu blinking in ie       1\n",
       "1371                          wrap stray text in p tags      1\n",
       "1527                       wordpress menu of categories      1\n",
       "1210                wordpress menu with superslide show      1\n",
       "1235  getting post information outside the wordpress...      1\n",
       "872                          why cant i include a blog       1\n",
       "1986  d slideshow for wordpress and url file access ...      1\n",
       "1902             submit wordpress form programmatically      1\n",
       "1947  move wordpress from home web server to web ser...      1\n",
       "...                                                 ...    ...\n",
       "99    how to use sqlab xpert tuning to tune sql for ...      2\n",
       "54    oracle connection problem on mac osx status fa...      2\n",
       "923               xaconnection performance in oracle g       2\n",
       "978   what is the pl sql api difference between orac...      2\n",
       "504          how do i call an oracle function from oci       2\n",
       "63                  explain plan cost vs execution time      2\n",
       "74    user interface for creating oracle sql loader ...      2\n",
       "518   is it possible to refer to column names via bi...      2\n",
       "1117  can i run an arbitrary oracle sql script throu...      2\n",
       "463                  oracle logon protocol o logon in g      2\n",
       "255                                    databases oracle      2\n",
       "968   resultset logic when selecting tables without ...      2\n",
       "537   oracle stored procedures sys refcursor and nhi...      2\n",
       "235   oracle express edition can not connect remotel...      2\n",
       "1051  performance of remote materialized views in or...      2\n",
       "1065              merge output cursor of sp into table       2\n",
       "277               oracle logon trigger not being fired       2\n",
       "453       understanding lob segments sys lob in oracle       2\n",
       "838   oracle database character set issue with the a...      2\n",
       "709   finding the days of the week within a date ran...      2\n",
       "591                how to convert sql server to oracle       2\n",
       "48    how can i avoid ta warning fom an unused param...      2\n",
       "786   oracle hierarchical query how to include top l...      2\n",
       "318   oracle record history using as of timestamp wi...      2\n",
       "515   how to call a function with rowtype parameter ...      2\n",
       "33                        ssis oracle parameter mapping      2\n",
       "434    multi line pl sql command with net oraclecommand      2\n",
       "577   read write data from to a file in pl sql witho...      2\n",
       "327   oracle sql parsing a name string and convertin...      2\n",
       "830   oracle optimizing query involving date calcula...      2\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.152530Z",
     "start_time": "2019-09-10T08:31:14.147953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:31:14.621358Z",
     "start_time": "2019-09-10T08:31:14.155615Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train['Text']\n",
    "y_train = train['Label'].values\n",
    "X_test = test['Text']\n",
    "y_test = test['Label'].values\n",
    "\n",
    "X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))\n",
    "X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))\n",
    "\n",
    "X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)\n",
    "X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:58:59.333032Z",
     "start_time": "2019-09-09T14:58:59.298243Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    \n",
    "    synonyms = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym) \n",
    "    \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    \n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        \n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            #print(\"replaced\", random_word, \"with\", synonym)\n",
    "            num_replaced += 1\n",
    "        \n",
    "        if num_replaced >= n: #only replace up to n words\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def iterative_replace(df):\n",
    "    \n",
    "    df = df.reset_index().drop(['index'], axis=1)\n",
    "    index_row = df.index\n",
    "    df_2 = pd.DataFrame()\n",
    "    \n",
    "    for row in index_row:\n",
    "        for k in range(1,6):\n",
    "            df_2 = df_2.append({'Text':synonym_replacement(df.loc[row]['Text'], k), 'Label':df.loc[row]['Label']}, ignore_index=True)\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:58:59.695868Z",
     "start_time": "2019-09-09T14:58:59.675230Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_deletion(words, p):\n",
    "\n",
    "    words = words.split()\n",
    "    \n",
    "    #obviously, if there's only one word, don't delete it\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    #randomly delete words with probability p\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    #if you end up deleting all words, just return a random word\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def iterative_delete(df):\n",
    "    \n",
    "    df = df.reset_index().drop(['index'], axis=1)\n",
    "    index_row = df.index\n",
    "    df_2 = pd.DataFrame()\n",
    "    \n",
    "    for row in index_row:\n",
    "        df_2 = df_2.append({'Text':random_deletion(df.loc[row]['Text'], 0.25), 'Label':df.loc[row]['Label']}, ignore_index=True)\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:59:00.168295Z",
     "start_time": "2019-09-09T14:59:00.144499Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_swap(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "        \n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def swap_word(new_words):\n",
    "    \n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    \n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        \n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    \n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "    return new_words\n",
    "\n",
    "def iterative_swap(df):\n",
    "    \n",
    "    df = df.reset_index().drop(['index'], axis=1)\n",
    "    index_row = df.index\n",
    "    df_2 = pd.DataFrame()\n",
    "    for row in index_row:\n",
    "        df_2 = df_2.append({'Text':random_swap(df.loc[row]['Text'], 2), 'Label':df.loc[row]['Label']}, ignore_index=True)\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:59:01.181955Z",
     "start_time": "2019-09-09T14:59:01.152899Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_insertion(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "        \n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "def add_word(new_words):\n",
    "    \n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    \n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "        \n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n",
    "    \n",
    "def iterative_insert(df):\n",
    "    \n",
    "    df = df.reset_index().drop(['index'], axis=1)\n",
    "    index_row = df.index\n",
    "    df_2 = pd.DataFrame()\n",
    "    \n",
    "    for row in index_row:\n",
    "        df_2 = df_2.append({'Text':random_insertion(df.loc[row]['Text'], 2), 'Label':df.loc[row]['Label']}, ignore_index=True)\n",
    "        \n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:59:04.924959Z",
     "start_time": "2019-09-09T14:59:03.141470Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_replace = iterative_replace(train)\n",
    "df_delete = iterative_delete(train)\n",
    "df_swap = iterative_swap(train)\n",
    "df_insert = iterative_insert(train)\n",
    "\n",
    "train = pd.concat([train, df_replace, df_delete, df_swap, df_insert], axis=0).reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T14:59:06.350551Z",
     "start_time": "2019-09-09T14:59:05.829293Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train['Text']\n",
    "y_train = train['Label'].values\n",
    "X_test = test['Text']\n",
    "y_test = test['Label'].values\n",
    "\n",
    "X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))\n",
    "X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))\n",
    "\n",
    "X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)\n",
    "X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities in PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:38:21.419881Z",
     "start_time": "2019-09-10T08:38:21.415733Z"
    }
   },
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:34:17.541159Z",
     "start_time": "2019-09-10T08:34:17.426574Z"
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool(nn.Module):\n",
    "    def __init__(self, dim=1):\n",
    "        super(MaxPool, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return torch.max(input, self.dim)[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ +'('+ 'dim=' + str(self.dim) + ')'\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, *sizes):\n",
    "        super(View, self).__init__()\n",
    "        self.sizes_list = sizes\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(*self.sizes_list)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'sizes=' + str(self.sizes_list) + ')'\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim1=0, dim2=1):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.transpose(self.dim1, self.dim2).contiguous()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + 'between=' + str(self.dim1) + ',' + str(self.dim2) + ')'\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size, w_hid_size, h_hid_size, win, batch_size,with_proj=False):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('transpose', Transpose())\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=emb_size)\n",
    "        self.model.add_module('emb', self.embed)\n",
    "        if with_proj:\n",
    "            self.model.add_module('view1', View(-1, emb_size))\n",
    "            self.model.add_module('linear1', nn.Linear(emb_size, w_hid_size))\n",
    "            self.model.add_module('relu1', nn.ReLU())\n",
    "        else:\n",
    "            w_hid_size = emb_size\n",
    "\n",
    "        self.model.add_module('trans2', Transpose(1, 2))\n",
    "\n",
    "        conv_nn = nn.Conv1d(w_hid_size, h_hid_size, win, padding=1)\n",
    "        self.model.add_module('conv', conv_nn)\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "\n",
    "        self.model.add_module('max', MaxPool(2))\n",
    "\n",
    "        self.model.add_module('view4', View(-1, h_hid_size))\n",
    "        self.model.add_module('linear2', nn.Linear(h_hid_size, num_labels))\n",
    "        self.model.add_module('softmax', nn.LogSoftmax())\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.model.forward(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:38:21.419881Z",
     "start_time": "2019-09-10T08:38:21.415733Z"
    }
   },
   "source": [
    "## Load BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:36:59.732744Z",
     "start_time": "2019-09-10T08:36:59.624085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "dependencies = ['torch', 'tqdm', 'boto3', 'requests', 'regex']\n",
    "\n",
    "def bertTokenizer(*args, **kwargs):\n",
    "    tokenizer = BertTokenizer.from_pretrained(*args, **kwargs)\n",
    "    return tokenizer\n",
    "\n",
    "def bertModel(*args, **kwargs):\n",
    "    model = BertModel.from_pretrained(*args, **kwargs)\n",
    "    return model\n",
    "\n",
    "def bertForNextSentencePrediction(*args, **kwargs):\n",
    "    model = BertForNextSentencePrediction.from_pretrained(*args, **kwargs)\n",
    "    return model\n",
    "\n",
    "def bertForPreTraining(*args, **kwargs):\n",
    "    model = BertForPreTraining.from_pretrained(*args, **kwargs)\n",
    "    return model\n",
    "\n",
    "def bertForMaskedLM(*args, **kwargs):\n",
    "    model = BertForMaskedLM.from_pretrained(*args, **kwargs)\n",
    "    return model\n",
    "\n",
    "def bertForSequenceClassification(*args, **kwargs):\n",
    "    model = BertForSequenceClassification.from_pretrained(*args, **kwargs)\n",
    "    return model\n",
    "\n",
    "def bertForMultipleChoice(*args, **kwargs):\n",
    "    model = BertForMultipleChoice.from_pretrained(*args, **kwargs)\n",
    "    return model\n",
    "\n",
    "def bertForQuestionAnswering(*args, **kwargs):\n",
    "    model = BertForQuestionAnswering.from_pretrained(*args, **kwargs)\n",
    "    return model\n",
    "\n",
    "def bertForTokenClassification(*args, **kwargs):\n",
    "    model = BertForTokenClassification.from_pretrained(*args, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path, tokenizer, seq_len, encoding=\"utf-8\", corpus_lines=None, on_memory=True):\n",
    "        self.vocab = tokenizer.vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.on_memory = on_memory\n",
    "        self.corpus_lines = corpus_lines  # number of non-empty lines in input corpus\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "        self.current_doc = 0  # to avoid random sentence from same doc\n",
    "\n",
    "        # for loading samples directly from file\n",
    "        self.sample_counter = 0  # used to keep track of full epochs on file\n",
    "        self.line_buffer = None  # keep second sentence of a pair in memory and use as first sentence in next pair\n",
    "\n",
    "        # for loading samples in memory\n",
    "        self.current_random_doc = 0\n",
    "        self.num_docs = 0\n",
    "        self.sample_to_doc = [] # map sample index to doc and line\n",
    "\n",
    "        # load samples into memory\n",
    "        if on_memory:\n",
    "            self.all_docs = []\n",
    "            doc = []\n",
    "            self.corpus_lines = 0\n",
    "            with open(corpus_path, \"r\", encoding=encoding) as f:\n",
    "                for line in tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "                    line = line.strip()\n",
    "                    if line == \"\":\n",
    "                        self.all_docs.append(doc)\n",
    "                        doc = []\n",
    "                        #remove last added sample because there won't be a subsequent line anymore in the doc\n",
    "                        self.sample_to_doc.pop()\n",
    "                    else:\n",
    "                        #store as one sample\n",
    "                        sample = {\"doc_id\": len(self.all_docs),\n",
    "                                  \"line\": len(doc)}\n",
    "                        self.sample_to_doc.append(sample)\n",
    "                        doc.append(line)\n",
    "                        self.corpus_lines = self.corpus_lines + 1\n",
    "\n",
    "            # if last row in file is not empty\n",
    "            if self.all_docs[-1] != doc:\n",
    "                self.all_docs.append(doc)\n",
    "                self.sample_to_doc.pop()\n",
    "\n",
    "            self.num_docs = len(self.all_docs)\n",
    "\n",
    "        # load samples later lazily from disk\n",
    "        else:\n",
    "            if self.corpus_lines is None:\n",
    "                with open(corpus_path, \"r\", encoding=encoding) as f:\n",
    "                    self.corpus_lines = 0\n",
    "                    for line in tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "                        if line.strip() == \"\":\n",
    "                            self.num_docs += 1\n",
    "                        else:\n",
    "                            self.corpus_lines += 1\n",
    "\n",
    "                    # if doc does not end with empty line\n",
    "                    if line.strip() != \"\":\n",
    "                        self.num_docs += 1\n",
    "\n",
    "            self.file = open(corpus_path, \"r\", encoding=encoding)\n",
    "            self.random_file = open(corpus_path, \"r\", encoding=encoding)\n",
    "\n",
    "    def __len__(self):\n",
    "        # last line of doc won't be used, because there's no \"nextSentence\". Additionally, we start counting at 0.\n",
    "        return self.corpus_lines - self.num_docs - 1\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        cur_id = self.sample_counter\n",
    "        self.sample_counter += 1\n",
    "        if not self.on_memory:\n",
    "            # after one epoch we start again from beginning of file\n",
    "            if cur_id != 0 and (cur_id % len(self) == 0):\n",
    "                self.file.close()\n",
    "                self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "\n",
    "        t1, t2, is_next_label = self.random_sent(item)\n",
    "\n",
    "        # tokenize\n",
    "        tokens_a = self.tokenizer.tokenize(t1)\n",
    "        tokens_b = self.tokenizer.tokenize(t2)\n",
    "\n",
    "        # combine to one sample\n",
    "        cur_example = InputExample(guid=cur_id, tokens_a=tokens_a, tokens_b=tokens_b, is_next=is_next_label)\n",
    "\n",
    "        # transform sample to features\n",
    "        cur_features = convert_example_to_features(cur_example, self.seq_len, self.tokenizer)\n",
    "\n",
    "        cur_tensors = (torch.tensor(cur_features.input_ids),\n",
    "                       torch.tensor(cur_features.input_mask),\n",
    "                       torch.tensor(cur_features.segment_ids),\n",
    "                       torch.tensor(cur_features.lm_label_ids),\n",
    "                       torch.tensor(cur_features.is_next))\n",
    "\n",
    "        return cur_tensors\n",
    "\n",
    "    def random_sent(self, index):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of two sentences. With prob. 50% these are two subsequent sentences\n",
    "        from one doc. With 50% the second sentence will be a random one from another doc.\n",
    "        :param index: int, index of sample.\n",
    "        :return: (str, str, int), sentence 1, sentence 2, isNextSentence Label\n",
    "        \"\"\"\n",
    "        t1, t2 = self.get_corpus_line(index)\n",
    "        t = random.random()\n",
    "        if t > 0.5:\n",
    "            label = 0\n",
    "        else:\n",
    "            t2 = self.get_random_line()\n",
    "            label = 1\n",
    "\n",
    "        assert len(t1) > 0\n",
    "        assert len(t2) > 0\n",
    "        return t1, t2, label\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        \"\"\"\n",
    "        Get one sample from corpus consisting of a pair of two subsequent lines from the same doc.\n",
    "        :param item: int, index of sample.\n",
    "        :return: (str, str), two subsequent sentences from corpus\n",
    "        \"\"\"\n",
    "        t1 = \"\"\n",
    "        t2 = \"\"\n",
    "        assert item < self.corpus_lines\n",
    "        if self.on_memory:\n",
    "            sample = self.sample_to_doc[item]\n",
    "            t1 = self.all_docs[sample[\"doc_id\"]][sample[\"line\"]]\n",
    "            t2 = self.all_docs[sample[\"doc_id\"]][sample[\"line\"]+1]\n",
    "            # used later to avoid random nextSentence from same doc\n",
    "            self.current_doc = sample[\"doc_id\"]\n",
    "            return t1, t2\n",
    "        else:\n",
    "            if self.line_buffer is None:\n",
    "                # read first non-empty line of file\n",
    "                while t1 == \"\" :\n",
    "                    t1 = next(self.file).strip()\n",
    "                    t2 = next(self.file).strip()\n",
    "            else:\n",
    "                # use t2 from previous iteration as new t1\n",
    "                t1 = self.line_buffer\n",
    "                t2 = next(self.file).strip()\n",
    "                # skip empty rows that are used for separating documents and keep track of current doc id\n",
    "                while t2 == \"\" or t1 == \"\":\n",
    "                    t1 = next(self.file).strip()\n",
    "                    t2 = next(self.file).strip()\n",
    "                    self.current_doc = self.current_doc+1\n",
    "            self.line_buffer = t2\n",
    "\n",
    "        assert t1 != \"\"\n",
    "        assert t2 != \"\"\n",
    "        return t1, t2\n",
    "\n",
    "    def get_random_line(self):\n",
    "        \"\"\"\n",
    "        Get random line from another document for nextSentence task.\n",
    "        :return: str, content of one line\n",
    "        \"\"\"\n",
    "        # Similar to original tf repo: This outer loop should rarely go for more than one iteration for large\n",
    "        # corpora. However, just to be careful, we try to make sure that\n",
    "        # the random document is not the same as the document we're processing.\n",
    "        for _ in range(10):\n",
    "            if self.on_memory:\n",
    "                rand_doc_idx = random.randint(0, len(self.all_docs)-1)\n",
    "                rand_doc = self.all_docs[rand_doc_idx]\n",
    "                line = rand_doc[random.randrange(len(rand_doc))]\n",
    "            else:\n",
    "                rand_index = random.randint(1, self.corpus_lines if self.corpus_lines < 1000 else 1000)\n",
    "                # rand_index = 892\n",
    "                #pick random line\n",
    "                for _ in range(rand_index):\n",
    "                    line = self.get_next_line()\n",
    "                \n",
    "            #check if our picked random line is really from another doc like we want it to be\n",
    "            if self.current_random_doc != self.current_doc:\n",
    "                \n",
    "                break\n",
    "        # print(\"random Index:\", rand_index, line)\n",
    "        return line\n",
    "\n",
    "    def get_next_line(self):\n",
    "        \"\"\" Gets next line of random_file and starts over when reaching end of file\"\"\"\n",
    "        try:\n",
    "            line = next(self.random_file).strip()\n",
    "\n",
    "            #keep track of which document we are currently looking at to later avoid having the same doc as t1\n",
    "            while line == \"\":\n",
    "                self.current_random_doc = self.current_random_doc + 1\n",
    "                line = next(self.random_file).strip()\n",
    "        except StopIteration:\n",
    "            self.random_file.close()\n",
    "            self.random_file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "            line = next(self.random_file).strip()\n",
    "        \n",
    "        return line\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for the language model.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, tokens_a, tokens_b=None, is_next=None, lm_labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            tokens_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            tokens_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.tokens_a = tokens_a\n",
    "        self.tokens_b = tokens_b\n",
    "        self.is_next = is_next  # nextSentence\n",
    "        self.lm_labels = lm_labels  # masked words for language model\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, is_next, lm_label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_next = is_next\n",
    "        self.lm_label_ids = lm_label_ids\n",
    "\n",
    "\n",
    "def random_word(tokens, tokenizer):\n",
    "    \"\"\"\n",
    "    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n",
    "    :param tokens: list of str, tokenized sentence.\n",
    "    :param tokenizer: Tokenizer, object used for tokenization (we need it's vocab here)\n",
    "    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n",
    "    \"\"\"\n",
    "    output_label = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        prob = random.random()\n",
    "        # mask token with 15% probability\n",
    "        if prob < 0.15:\n",
    "            prob /= 0.15\n",
    "\n",
    "            # 80% randomly change token to mask token\n",
    "            if prob < 0.8:\n",
    "                tokens[i] = \"[MASK]\"\n",
    "\n",
    "            # 10% randomly change token to random token\n",
    "            elif prob < 0.9:\n",
    "                tokens[i] = random.choice(list(tokenizer.vocab.items()))[0]\n",
    "\n",
    "            # -> rest 10% randomly keep current token\n",
    "\n",
    "            # append current token to output (we will predict these later)\n",
    "            try:\n",
    "                output_label.append(tokenizer.vocab[token])\n",
    "            except KeyError:\n",
    "                # For unknown words (should not occur with BPE vocab)\n",
    "                output_label.append(tokenizer.vocab[\"[UNK]\"])\n",
    "                logger.warning(\"Cannot find token '{}' in vocab. Using [UNK] insetad\".format(token))\n",
    "        else:\n",
    "            # no masking token (will be ignored by loss function later)\n",
    "            output_label.append(-1)\n",
    "\n",
    "    return tokens, output_label\n",
    "\n",
    "\n",
    "def convert_example_to_features(example, max_seq_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with\n",
    "    IDs, LM labels, input_mask, CLS and SEP tokens etc.\n",
    "    :param example: InputExample, containing sentence input as strings and is_next label\n",
    "    :param max_seq_length: int, maximum length of sequence.\n",
    "    :param tokenizer: Tokenizer\n",
    "    :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training)\n",
    "    \"\"\"\n",
    "    tokens_a = example.tokens_a\n",
    "    tokens_b = example.tokens_b\n",
    "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "    # length is less than the specified length.\n",
    "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "    tokens_a, t1_label = random_word(tokens_a, tokenizer)\n",
    "    tokens_b, t2_label = random_word(tokens_b, tokenizer)\n",
    "    # concatenate lm labels and account for CLS, SEP, SEP\n",
    "    lm_label_ids = ([-1] + t1_label + [-1] + t2_label + [-1])\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0   0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    assert len(tokens_b) > 0\n",
    "    for token in tokens_b:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(1)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        lm_label_ids.append(-1)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(lm_label_ids) == max_seq_length\n",
    "\n",
    "    if example.guid < 5:\n",
    "        logger.info(\"*** Example ***\")\n",
    "        logger.info(\"guid: %s\" % (example.guid))\n",
    "        logger.info(\"tokens: %s\" % \" \".join(\n",
    "                [str(x) for x in tokens]))\n",
    "        logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        logger.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        logger.info(\"LM label: %s \" % (lm_label_ids))\n",
    "        logger.info(\"Is next sentence label: %s \" % (example.is_next))\n",
    "\n",
    "    features = InputFeatures(input_ids=input_ids,\n",
    "                             input_mask=input_mask,\n",
    "                             segment_ids=segment_ids,\n",
    "                             lm_label_ids=lm_label_ids,\n",
    "                             is_next=example.is_next)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T08:38:21.419881Z",
     "start_time": "2019-09-10T08:38:21.415733Z"
    }
   },
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTLField(Field):\n",
    "\n",
    "    def __init__(\n",
    "            self, **kwargs):\n",
    "        super(MTLField, self).__init__(**kwargs)\n",
    "\n",
    "    def build_vocab(self, dataset_list, **kwargs):\n",
    "        ## Load BERT\n",
    "        counter = Counter()\n",
    "        sources = []\n",
    "        for arg in dataset_list:\n",
    "            if isinstance(arg, Dataset):\n",
    "                sources += [getattr(arg, name) for name, field in\n",
    "                            arg.fields.items() if field is self]\n",
    "            else:\n",
    "                sources.append(arg)\n",
    "        for data in sources:\n",
    "            for x in data:\n",
    "                if not self.sequential:\n",
    "                    x = [x]\n",
    "                counter.update(x)\n",
    "        specials = list(OrderedDict.fromkeys(\n",
    "            tok for tok in [self.pad_token, self.init_token, self.eos_token]\n",
    "            if tok is not None))\n",
    "        self.vocab = Vocab(counter, specials=specials, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, glob, random\n",
    "import time\n",
    "import parser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from AdaAdam import AdaAdam\n",
    "import torch.optim as OPT\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm, trange\n",
    "import logging\n",
    "\n",
    "from torchtext import data\n",
    "import DataProcessing\n",
    "from DataProcessing.MLTField import MTLField\n",
    "from DataProcessing.NlcDatasetSingleFile import NlcDatasetSingleFile\n",
    "from CNNModel import CNNModel\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO )\n",
    "batch_size = 10\n",
    "seed = 12345678\n",
    "torch.manual_seed(seed)\n",
    "Train = False\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_train_test_files(listfilename, test_suffix='.test'):\n",
    "    filein = open(listfilename, 'r')\n",
    "    file_tuples = []\n",
    "    task_classes = ['.t2', '.t4', '.t5']\n",
    "    for line in filein:\n",
    "        array = line.strip().split('\\t')\n",
    "        line = array[0]\n",
    "        for t_class in task_classes:\n",
    "            trainfile = line + t_class + '.train'\n",
    "            devfile = line + t_class + '.dev'\n",
    "            testfile = line + t_class + test_suffix\n",
    "            file_tuples.append((trainfile, devfile, testfile))\n",
    "    filein.close()\n",
    "    return file_tuples\n",
    "\n",
    "filelist = 'data/Amazon_few_shot/workspace.filtered.list'\n",
    "targetlist = 'data/Amazon_few_shot/workspace.target.list'\n",
    "workingdir = 'data/Amazon_few_shot'\n",
    "emfilename = 'glove.6B.300d'\n",
    "emfiledir = '..'\n",
    "\n",
    "datasets = []\n",
    "list_datasets = []\n",
    "\n",
    "\n",
    "file_tuples = load_train_test_files(filelist)\n",
    "print(file_tuples)\n",
    "\n",
    "TEXT = MTLField(lower=True)\n",
    "for (trainfile, devfile, testfile) in file_tuples:\n",
    "    print(trainfile, devfile, testfile)\n",
    "    LABEL1 = data.Field(sequential=False)\n",
    "    train1, dev1, test1 = NlcDatasetSingleFile.splits(\n",
    "        TEXT, LABEL1, path=workingdir, train=trainfile,\n",
    "        validation=devfile, test=testfile)\n",
    "    datasets.append((TEXT, LABEL1, train1, dev1, test1))\n",
    "    list_datasets.append(train1)\n",
    "    list_datasets.append(dev1)\n",
    "    list_datasets.append(test1)\n",
    "\n",
    "target_datasets = []\n",
    "target_file = load_train_test_files(targetlist)\n",
    "print(target_file)\n",
    "\n",
    "for (trainfile, devfile, testfile) in target_file:\n",
    "    print(trainfile, devfile, testfile)\n",
    "    LABEL2 = data.Field(sequential=False)\n",
    "    train2, dev2, test2 = NlcDatasetSingleFile.splits(TEXT, LABEL2, path=workingdir, \n",
    "    train=trainfile,validation=devfile, test=testfile)\n",
    "    target_datasets.append((TEXT, LABEL2, train2, dev2, test2))\n",
    "\n",
    "    \n",
    "\n",
    "datasets_iters = []\n",
    "for (TEXT, LABEL, train, dev, test) in datasets:\n",
    "    train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, dev, test), batch_size=batch_size, device=device,shuffle=True)\n",
    "    train_iter.repeat = False\n",
    "    datasets_iters.append((train_iter, dev_iter, test_iter))\n",
    "\n",
    "fsl_ds_iters = []\n",
    "for (TEXT, LABEL, train, dev, test) in target_datasets:\n",
    "    train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train,dev, test), batch_size=batch_size, device=device)\n",
    "    train_iter.repeat = False\n",
    "    fsl_ds_iters.append((train_iter, dev_iter, test_iter))\n",
    "\n",
    "num_batch_total = 0\n",
    "for i, (TEXT, LABEL, train, dev, test) in enumerate(datasets):\n",
    "    # print('DATASET%d'%(i+1))\n",
    "    # print('train.fields', train.fields)\n",
    "    # print('len(train)', len(train))\n",
    "    # print('len(dev)', len(dev))\n",
    "    # print('len(test)', len(test))\n",
    "    # print('vars(train[0])', vars(train[0]))\n",
    "    num_batch_total += len(train) / batch_size\n",
    "\n",
    "TEXT.build_vocab(list_datasets, vectors = emfilename, vectors_cache = emfiledir)\n",
    "# TEXT.build_vocab(list_dataset)\n",
    "\n",
    "# build the vocabulary\n",
    "for taskid, (TEXT, LABEL, train, dev, test) in enumerate(datasets):\n",
    "    LABEL.build_vocab(train, dev, test)\n",
    "    LABEL.vocab.itos = LABEL.vocab.itos[1:]\n",
    "\n",
    "    for k, v in LABEL.vocab.stoi.items():\n",
    "        LABEL.vocab.stoi[k] = v - 1\n",
    "\n",
    "    # print vocab information\n",
    "    # print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "    # print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n",
    "\n",
    "    # print(LABEL.vocab.itos)\n",
    "    # print(len(LABEL.vocab.itos))\n",
    "\n",
    "    # print(len(LABEL.vocab.stoi))\n",
    "fsl_num_tasks = 0\n",
    "for taskid, (TEXT, LABEL, train, dev, test) in enumerate(target_datasets):\n",
    "    fsl_num_tasks += 1\n",
    "    LABEL.build_vocab(train, dev, test)\n",
    "    LABEL.vocab.itos = LABEL.vocab.itos[1:]\n",
    "    for k, v in LABEL.vocab.stoi.items():\n",
    "        LABEL.vocab.stoi[k] = v - 1\n",
    "\n",
    "nums_embed = len(TEXT.vocab)\n",
    "dim_embed = 100\n",
    "dim_w_hid = 200\n",
    "dim_h_hid = 100\n",
    "Inner_lr = 2e-6\n",
    "Outer_lr = 1e-5\n",
    "\n",
    "n_labels = []\n",
    "for (TEXT, LABEL, train, dev, test) in datasets:\n",
    "   n_labels.append(len(LABEL.vocab))\n",
    "print(n_labels)\n",
    "num_tasks = len(n_labels)\n",
    "print(\"num_tasks\", num_tasks)\n",
    "winsize = 3\n",
    "num_labels = len(LABEL.vocab.itos)\n",
    "model = CNNModel(nums_embed, num_labels, dim_embed, dim_w_hid, dim_h_hid, winsize, batch_size)\n",
    "\n",
    "print(\"GPU Device: \", device)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = OPT.Adam(model.parameters(), lr=Inner_lr)\n",
    "Inner_epochs = 4\n",
    "epochs = 2\n",
    "\n",
    "N_task = 5\n",
    "\n",
    "task_list = np.arange(num_tasks)\n",
    "print(\"Total Batch: \", num_batch_total)\n",
    "output_model_file = '/tmp/CNN_MAML_output'\n",
    "if Train:\n",
    "    for t in trange(int(num_batch_total*epochs/Inner_epochs), desc=\"Iterations\"):\n",
    "        selected_task = np.random.choice(task_list, N_task,replace=False)\n",
    "        weight_before = deepcopy(model.state_dict())\n",
    "        update_vars = []\n",
    "        fomaml_vars = []\n",
    "        for task_id in selected_task:\n",
    "            # print(task_id)\n",
    "            (train_iter, dev_iter, test_iter) = datasets_iters[task_id]\n",
    "            train_iter.init_epoch()\n",
    "            model.train()\n",
    "            n_correct = 0\n",
    "            n_step = 0\n",
    "            for inner_iter in range(Inner_epochs):\n",
    "                batch = next(iter(train_iter))\n",
    "\n",
    "                # print(batch.text)\n",
    "                # print(batch.label)\n",
    "                logits = model(batch.text)\n",
    "                loss = criterion(logits.view(-1, num_labels), batch.label.data.view(-1))\n",
    "                \n",
    "\n",
    "                n_correct = (torch.max(logits, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "                n_step = batch.batch_size\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "            task_acc = 100.*n_correct/n_step\n",
    "            if t%10 == 0:\n",
    "                logger.info(\"Iter: %d, task id: %d, train acc: %f\", t, task_id, task_acc)\n",
    "            weight_after = deepcopy(model.state_dict())\n",
    "            update_vars.append(weight_after)\n",
    "            model.load_state_dict(weight_before)\n",
    "\n",
    "        new_weight_dict = {}\n",
    "        for name in weight_before:\n",
    "            weight_list = [tmp_weight_dict[name] for tmp_weight_dict in update_vars]\n",
    "            weight_shape = list(weight_list[0].size())\n",
    "            stack_shape = [len(weight_list)] + weight_shape\n",
    "            stack_weight = torch.empty(stack_shape)\n",
    "            for i in range(len(weight_list)):\n",
    "                stack_weight[i,:] = weight_list[i] \n",
    "            new_weight_dict[name] = torch.mean(stack_weight, dim=0).cuda()\n",
    "            new_weight_dict[name] = weight_before[name]+(new_weight_dict[name]-weight_before[name])/Inner_lr*Outer_lr\n",
    "        model.load_state_dict(new_weight_dict)\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), output_model_file)\n",
    "\n",
    "model.load_state_dict(torch.load(output_model_file))\n",
    "logger.info(\"***** Running evaluation *****\")\n",
    "fsl_task_list = np.arange(fsl_num_tasks)\n",
    "weight_before = deepcopy(model.state_dict())\n",
    "fsl_epochs = 3\n",
    "Total_acc = 0\n",
    "opt = OPT.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "for task_id in fsl_task_list:\n",
    "    model.train()\n",
    "    (train_iter, dev_iter, test_iter) = fsl_ds_iters[task_id]\n",
    "    train_iter.init_epoch()\n",
    "    batch = next(iter(train_iter))\n",
    "    for i in range(fsl_epochs):\n",
    "        logits = model(batch.text)\n",
    "        loss = criterion(logits.view(-1, num_labels), batch.label.data.view(-1))\n",
    "        n_correct = (torch.max(logits, 1)[1].view(batch.label.size()).data == batch.label.data).sum()\n",
    "        n_size = batch.batch_size\n",
    "        train_acc = 100. * n_correct / n_size\n",
    "        loss = criterion(logits.view(-1, num_labels), batch.label.data.view(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        logger.info(\"  Task id: %d, fsl epoch: %d, Acc: %f, loss: %f\", task_id, i, train_acc, loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_iter.init_epoch()\n",
    "    n_correct = 0\n",
    "    n_size = 0\n",
    "    for test_batch_idx, test_batch in enumerate(test_iter):\n",
    "        with torch.no_grad():\n",
    "            logits = model(test_batch.text)\n",
    "        loss = criterion(logits.view(-1, num_labels), test_batch.label.data.view(-1))\n",
    "        n_correct += (torch.max(logits, 1)[1].view(test_batch.label.size()).data == test_batch.label.data).sum()\n",
    "        n_size += test_batch.batch_size\n",
    "    test_acc = 100.* n_correct/n_size\n",
    "    logger.info(\"FSL test Number: %d, Accuracy: %f\",n_size, test_acc)\n",
    "    Total_acc += test_acc\n",
    "    model.load_state_dict(weight_before)\n",
    "\n",
    "print(\"Mean Accuracy is : \", float(Total_acc)/fsl_num_tasks)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
