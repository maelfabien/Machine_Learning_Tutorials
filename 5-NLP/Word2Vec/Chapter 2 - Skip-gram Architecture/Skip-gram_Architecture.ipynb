{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "--------------------\n",
    "\n",
    "In this notebook, we'll reinforce our understanding of the skip-gram neural network architecture by implementing it from scratch. We'll stop at just the feed-forward implementation--that is, we'll be able to evaluate the network on an input, but we won't be implementing back-propagation from scratch here.\n",
    "\n",
    "# Contents\n",
    "\n",
    "----------------------\n",
    "\n",
    "* [Feed-Forward Implementation](#feed_forward)\n",
    "    * [Pre-trained Model](#model)\n",
    "    * [Notation](#notation)\n",
    "    * [Run Layer-by-Layer](#layer-by-layer)\n",
    "    * [Inspecting Network Output](#inspect_output)\n",
    "* [Additional Topics](#additional)\n",
    "    * [Is the one-hot vector necessary?](#one_hot)\n",
    "    * [Why no activation function on the projection layer?](#projection_activation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Implementation <a name=\"feed_forward\"></a>\n",
    "\n",
    "------------------\n",
    "\n",
    "We're going to take the weights from a pre-trained model and execute a forward pass on the network as an illustration of the architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model <a name=\"model\"></a>\n",
    "\n",
    "------------------\n",
    "\n",
    "To try something new, we'll use a different pre-trained model in this notebook. The model we'll be using comes from a nice code sample by [Kavita Ganesan](http://kavita-ganesan.com). She trains word2vec (using `gensim`) on a dataset of hotel reviews (~250k reviews, ~41.5M words) and chose to train 150 features (versus the 300 in the Google News model). Her model gets great word representations for adjectives like \"dirty\", \"polite\", and others that you'd expect to find in reviews.\n",
    "\n",
    "You can find her code [here](https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/word2vec) if you're interested, but for this notebook I've exported the vocabulary and trained model parameters and made them available to download. Run the next cell to download them, or download them manually from the following links:\n",
    "\n",
    "* [projection_weights.npy](https://drive.google.com/file/d/1s-Ndz2PcHMVFOZ8AsgIb8WaEmlNnecQP) ~40MB\n",
    "* [output_weights.npy](https://drive.google.com/file/d/10mpIhCU6FJdGjgGynU_qmphQt2rDTtDS) ~40MB\n",
    "* [index2word.p](https://drive.google.com/file/d/1goIP_NmKI3D1bprQVFNSP7zOwebt5dna) ~1MB\n",
    "* [word2index.p](https://drive.google.com/file/d/1hV-VEscKJTJFTWm5Kkl1XRY26YvH-eMp) ~4MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files...\n",
      "    ./data/projection_weights.npy\n",
      "    ./data/output_weights.npy\n",
      "    ./data/index2word.p\n",
      "    ./data/word2index.p\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Create the /data/ subdirectory if needed.\n",
    "if not os.path.exists('./data/'):\n",
    "    print(\"Making directory /data/\")\n",
    "    os.makedirs('./data/')\n",
    "\n",
    "# URLs and filenames for the data.    \n",
    "files = [\n",
    "    (\"https://drive.google.com/file/d/1s-Ndz2PcHMVFOZ8AsgIb8WaEmlNnecQP\", \n",
    "     \"./data/projection_weights.npy\"),\n",
    "    \n",
    "    (\"https://drive.google.com/file/d/10mpIhCU6FJdGjgGynU_qmphQt2rDTtDS\", \n",
    "     \"./data/output_weights.npy\"),\n",
    "    \n",
    "    (\"https://drive.google.com/file/d/1goIP_NmKI3D1bprQVFNSP7zOwebt5dna\", \n",
    "     \"./data/index2word.p\"),\n",
    "    \n",
    "    (\"https://drive.google.com/file/d/1hV-VEscKJTJFTWm5Kkl1XRY26YvH-eMp\", \n",
    "     \"./data/word2index.p\"),\n",
    "]\n",
    "\n",
    "print(\"Downloading files...\")\n",
    "\n",
    "# Download each of the files, about 85MB total.\n",
    "for file in files:\n",
    "    print(\"    \" + file[1])\n",
    "    \n",
    "    r = requests.get(file[0], allow_redirects=True)\n",
    "    open(file[1], 'wb').write(r.content)\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "Read in the vocabulary and the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading vocabulary...\n",
      "\n",
      "    Vocabulary is 70,537 words.\n",
      "\n",
      "\n",
      "Loading weight matrices...\n",
      "\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nLoading vocabulary...\")\n",
    "\n",
    "# Read in the list of vocabulary words.\n",
    "vocab_list = pickle.load(open('./data/index2word.p', 'rb'))\n",
    "\n",
    "# Read in the dictionary which maps words to their indeces \n",
    "# in the weight matrices.\n",
    "vocab = pickle.load(open('./data/word2index.p', 'rb'))\n",
    "\n",
    "# Report the size of the vocabulary.\n",
    "vocab_size = len(vocab_list)\n",
    "print('\\n    Vocabulary is {:,} words.\\n'.format(vocab_size))\n",
    "\n",
    "print(\"\\nLoading weight matrices...\\n\")\n",
    "# Load the weight matrices for the projection layer and the\n",
    "# output layer.\n",
    "W_proj = np.load('./data/projection_weights.npy')\n",
    "W_out = np.load('./data/output_weights.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation <a name=\"notation\"></a>\n",
    "\n",
    "---------------------\n",
    "\n",
    "We're going to run through the network one layer at a time, and see what comes out. We'll run it for my favorite word, \"couch\". :)\n",
    "\n",
    "To help interpret the linear algebra in a neural network, I like to print the matrix dimensions at each step.\n",
    "\n",
    "Below are a couple helper functions that I'll use to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Projection layer weights: [70,537  x  150]\n",
      "    Output layer weights: [70,537  x  150]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def shape_to_str(shape):\n",
    "    '''\n",
    "    Prints out the dimensions of a matrix neatly.\n",
    "    '''\n",
    "    return \"[{:,}  x  {:,}]\".format(shape[0], shape[1])\n",
    "\n",
    "\n",
    "def print_matrix_mult(x, x_name, y, y_name, z_name):\n",
    "    '''\n",
    "    Prints out the dimensions of a matrix multiplication,\n",
    "    x * y = z\n",
    "    '''\n",
    "    z_shape = (x.shape[0], y.shape[1])\n",
    "    \n",
    "    print(\"%16s  *  %16s  =  %10s\" % (x_name, y_name, z_name))\n",
    "    \n",
    "    print(\"%16s  *  %16s  =  %10s\" % (shape_to_str(x.shape), \n",
    "                                      shape_to_str(y.shape), \n",
    "                                      shape_to_str(z_shape)))\n",
    "\n",
    "# Let's use the first function to print out the weight dimensions.\n",
    "print('')\n",
    "print('    Projection layer weights: %s' % shape_to_str(W_proj.shape))\n",
    "print('    Output layer weights: %s\\n' % shape_to_str(W_out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "I'm using the following variable name conventions. For each of these, I append the name of the layer, \"proj\" or \"out\".\n",
    "\n",
    "* `W` - Neuron parameters (\"weights\")\n",
    "* `z` - Dot product between the output of the previous layer and the next layer's weights.\n",
    "    * e.g., `z_out = a_proj * W_out`\n",
    "* `a` - Activation values for a layer.\n",
    "    * e.g., `a_out = softmax(z_out)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Layer-by-Layer <a name=\"layer-by-layer\"></a>\n",
    "\n",
    "-----------------------\n",
    "\n",
    "**Step 1 - Input Layer**\n",
    "\n",
    "Create our input vector--a one-hot vector for the word \"couch\".\n",
    "\n",
    "*Side note: You may recall from the book that the one-hot vector isn't really necessary. We'll use it for now, though, and come back later to prove that it's unneeded.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the one-hot as a row vector with all zeros.\n",
    "one_hot = np.zeros( shape=(1, vocab_size) )\n",
    "\n",
    "# Look up the index for \"couch\" and set it to 1.\n",
    "one_hot[0, vocab[\"couch\"].index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 - Projection Layer**\n",
    "\n",
    "Feed the input vector into the first layer, which if you recall, has no activation function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input --> Projection Layer\n",
      "         one_hot  *            W_proj  =      z_proj\n",
      "  [1  x  70,537]  *  [70,537  x  150]  =  [1  x  150]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInput --> Projection Layer\")\n",
    "print_matrix_mult(one_hot, \"one_hot\", \n",
    "                  W_proj, \"W_proj\", \n",
    "                  \"z_proj\")\n",
    "\n",
    "# Multiply the one hot vector with the projection layer weights.\n",
    "z_proj = np.dot(one_hot, W_proj)\n",
    "\n",
    "# There is no activation function on the projection layer, so the\n",
    "# output of this layer is just the dot-product from above.\n",
    "a_proj = z_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 - Output Layer**\n",
    "\n",
    "Feed the output of the projection layer (which is actually just the word vector for \"couch\"!) into the output layer.\n",
    "\n",
    "The output layer uses the softmax activation function. This function takes the exponential of a neuron's output, and divides it by the exponentials of all the output neurons.\n",
    "\n",
    "$ S \\left( x_i\\right) = \\frac{\\displaystyle e^{x_i}}{\\displaystyle\\sum^n_{i=1}{e^{x_i}}} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Projection Layer --> Output Layer\n",
      "\n",
      "          a_proj  *            W_out'  =       z_out\n",
      "     [1  x  150]  *  [150  x  70,537]  =  [1  x  70,537]\n",
      "\n",
      "Output Activation...\n",
      "\n",
      "           a_out\n",
      "  [1  x  70,537]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProjection Layer --> Output Layer\\n\")\n",
    "print_matrix_mult(a_proj, \"a_proj\", \n",
    "                  W_out.T, \"W_out'\", \n",
    "                  \"z_out\")\n",
    "\n",
    "# Multiply the output of the projection layer with the \n",
    "# output layer weights.\n",
    "z_out = np.dot(a_proj, W_out.T)\n",
    "\n",
    "print(\"\\nOutput Activation...\\n\")\n",
    "\n",
    "# Apply the softmax function to the outputs.\n",
    "a_out = np.exp(z_out) / np.sum(np.exp(z_out))\n",
    "\n",
    "print(\"%16s\" % \"a_out\")\n",
    "print(\"%16s\" % shape_to_str(a_out.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Network Output <a name=\"inspect_output\"></a>\n",
    "\n",
    "-----------\n",
    "\n",
    "Now we have the distribution of context words for \"couch\"! Let's explore it!\n",
    "\n",
    "Does the distribution sum to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of outputs: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Does it sum to 1.0 as it should?\n",
    "print(\"Sum of outputs: %.2f\" % np.sum(a_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I find the word \"couch\" in some text, what are the most likely words to find around it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            --Word--   --Output--\n",
      "             pullout   0.6606\n",
      "             sleeper   0.3237\n",
      "               couch   0.008325\n",
      "                sofa   0.002821\n",
      "             foldout   0.00166\n",
      "                pull   0.001434\n",
      "              chairs   0.0004315\n",
      "           sectional   0.0003964\n",
      "            cushions   9.422E-05\n",
      "            loveseat   9.185E-05\n"
     ]
    }
   ],
   "source": [
    "a_out = a_out.flatten()\n",
    "\n",
    "# Sort the activations but return the sorted *indeces*.\n",
    "# This sorts them in ascending order.\n",
    "indeces = np.argsort(a_out)\n",
    "\n",
    "# Reverse the order to descending with some ugly Python.\n",
    "indeces = indeces[::-1]\n",
    "\n",
    "print('%20s   %s' % ('--Word--', '--Output--'))\n",
    "\n",
    "# For the most likely context words...\n",
    "for i in range(0, 10):\n",
    "    # Get the word index for result 'i' (in reverse order).\n",
    "    word_index = indeces[i]\n",
    "    \n",
    "    # Lookup the word.\n",
    "    word = vocab_list[word_index]\n",
    "    \n",
    "    # Lookup the output value.\n",
    "    a_out_i = a_out[word_index]\n",
    "    \n",
    "    # Print the word and its output.\n",
    "    print('%20s   %.4G' % (word, a_out_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are very reasonable results! Remember that this model was trained on hotel reviews, so \"couch\" is a very relevant word in this model. You can imagine how all of the above might appear near couch: \"pullout couch\", \"sleeper couch\", \"foldout couch\", \"sectional couch\", \"couch cushions\", \"couch and loveseat\".\n",
    "\n",
    "You can even imagine how the word \"couch\" might appear near itself (the original word2vec C code doesn't appear to do anything to prevent training the input word as a context word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Topics <a name=\"additional\"></a>\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the one-hot vector necessary? <a name=\"one_hot\"></a>\n",
    "---------\n",
    "\n",
    "Recall from the book that the one-hot vector is really only part of the mathematical formulation, and not at all necessary in the implementation. Just to prove this to ourselves, we'll select the word vector couch, and observe that it's identical to the output of the projection layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between `z_proj` and `vec_couch` = 0.00\n"
     ]
    }
   ],
   "source": [
    "# Look up the word vector for couch.\n",
    "vec_couch = W_proj[vocab[\"couch\"].index, :]\n",
    "\n",
    "# Compare the word vector for \"couch\" with the output of \n",
    "# the projection layer using the one-hot vector. Calculate\n",
    "# the distance between them to check for equality.\n",
    "print(\"Distance between `z_proj` and `vec_couch` = %.2f\" % \n",
    "          np.linalg.norm(vec_couch - z_proj.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why no activation function on the projection layer? <a name=\"projection_activation\"></a>\n",
    "\n",
    "--------\n",
    "\n",
    "The hidden layer of this architecture is referred to as a \"projection\" layer because it has no activation function, as normal neural network layers do.\n",
    "\n",
    "Why not? I have to admit I don't know what would happen if you added an activation function, but I can at least demonstrate why it could make sense that an activation fucntion isn't needed. \n",
    "\n",
    "Let's look at a single neuron from the projection layer to try to understand this. Remember, a single neuron in this network represents a single word vector *feature*, not a vocabulary word... There are 150 projection layer neurons in this network, and each neuron has 70,537 weights (one for each word in the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed forward neuron 12...\n",
      "         one_hot  *           neur_12  =       z1_12\n",
      "  [1  x  70,537]  *    [70,537  x  1]  =   [1  x  1]\n",
      "\n",
      "  Neuron 12 output: 1.440549\n",
      "'Couch' feature 12: 1.440549\n"
     ]
    }
   ],
   "source": [
    "# Select the weights of an arbitrary neuron, number 12.\n",
    "# (The brackets around \"[12]\" force numpy to preserve \n",
    "# it as a 2D vector)\n",
    "neur_12 = W_proj[:, [12]]\n",
    "\n",
    "# Print the dimensions of this step.\n",
    "print(\"Feed forward neuron 12...\")\n",
    "print_matrix_mult(one_hot, \"one_hot\", \n",
    "                  neur_12, \"neur_12\", \"z1_12\")\n",
    "\n",
    "# Feed forward through this neuron.\n",
    "z_proj_12 = np.dot(one_hot, neur_12)\n",
    "\n",
    "# What's the output?\n",
    "print(\"\")\n",
    "print(\"  Neuron 12 output: %f\" % z_proj_12)\n",
    "print(\"'Couch' feature 12: %f\" % \n",
    "          W_proj[vocab[\"couch\"].index, 12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally a hidden layer neuron takes a linear combination of the input vector and the neuron's weights. We then have to introduce a non-linearity (such as sigmoid or ReLu) to give the network non-linear properties.\n",
    "\n",
    "Here, however, we're doing nothing more than selecting a value form the neuron's weights, *unmodified*. This \"projection\" layer doesn't actually do any computation, so the addition of an activation function seems pointless.\n",
    "\n",
    "Note that, for the same reason, the hidden layer doesn't include a bias term, either.\n",
    "\n",
    "You could perhaps think of the word2vec architecture as a single softmax layer, with the word vectors being the inputs to the network. In this case, however, we backpropagate to the training samples, and modify the *inputs* as part of the training! \n",
    "\n",
    "(It would be an interesting exercise, I think, to apply this to something like MNIST image classification--how would the images change if you backprogagated to the image vectors themselves?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
